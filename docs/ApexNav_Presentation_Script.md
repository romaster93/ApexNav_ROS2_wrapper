# ApexNav 발표 대본

> 발표 시간: 약 20분
> 대상: 로보틱스/AI 연구자
> 난이도: PhD 수준

---

## 슬라이드 1: 제목 (1분)

**[화면: 논문 제목, 저자, 소속 정보, ApexNav 로고]**

안녕하세요. 반갑습니다.

오늘 발표할 논문은 "ApexNAV: An Adaptive Exploration Strategy for Zero-Shot Object Navigation With Target-Centric Semantic Fusion"입니다.

이 논문은 홍콩과기대 광저우와 남방과기대의 Robotics-STAR Lab에서 2025년 IEEE Robotics and Automation Letters에 발표한 연구입니다.

**핵심 포인트:**
- 학습 없이 새로운 객체를 찾을 수 있는 Zero-Shot Object Navigation
- 비전-언어 모델 기반 적응형 탐색 전략
- 타겟 중심 의미론적 융합으로 VLM 환각 문제 해결

이 연구는 기존 VLFM 방법론보다 15-25% 향상된 성공률을 달성했습니다.

다음 슬라이드에서 문제 정의부터 시작하겠습니다.

---

## 슬라이드 2: 문제 정의 (2분)

**[화면: Zero-Shot Object Navigation 설명 다이어그램]**

먼저 우리가 해결하려는 문제를 정의하겠습니다.

**Zero-Shot Object Navigation의 정의:**
로봇이 특정 객체에 대한 사전 학습 없이, 오직 텍스트 설명만 주어진 상태에서 그 객체를 찾아 도달하는 작업입니다.

예를 들어, 로봇에게 "식탁을 찾아"라는 지시만 내려질 때, 로봇은 처음 보는 환경에서 식탁을 찾아야 합니다.

**이 문제가 중요한 이유:**
1. **실제 로봇 응용**: 검색 및 구조, 배송, 청소 로봇 등 실제 환경에서 필요
2. **일반화 가능성**: 새로운 객체나 환경에 적응해야 함
3. **학습 비용 제거**: 모든 객체에 대해 학습할 수 없음

**하지만 현실은 매우 어렵습니다.**

---

## 슬라이드 3: 기존 방법론의 한계 (2분)

**[화면: 기존 VLFM과 ApexNav의 비교 차트]**

기존 방법론, 특히 VLFM (Vision-Language Frontier Maps)의 한계점을 살펴보겠습니다.

**문제 1: VLM의 환각 (Hallucination)**

VLM은 한 번의 관찰로 객체를 판별하기 어렵습니다. 예를 들어:
- 의자의 검은 그림자를 검은색 가구로 오인
- 사람 형태의 물건을 실제 사람으로 오인
- 비슷한 텍스처의 객체를 같은 것으로 인식

이런 거짓 양성(False Positive) 검출이 탐색 효율을 크게 떨어뜨립니다.

**문제 2: 탐색 효율성 부족**

VLFM은 모든 프론티어에 동일한 가중치를 부여하거나, 단순 거리 기반 탐색만 수행합니다.
- 의미론적으로 가치 높은 영역을 우선순위에 두지 않음
- 환경과 상황에 따라 적응하지 못함
- 다중 프론티어 방문 시 경로 최적화 부족

**문제 3: 신뢰도 모델 부재**

기존 방법은:
- 카메라 시야각에서의 위치 정보를 무시
- 시간에 따른 누적 관찰을 제대로 활용하지 않음
- 일괄적인 임계값 적용으로 환경에 맞지 않음

따라서 우리는 이 세 가지 문제를 근본적으로 해결해야 했습니다.

---

## 슬라이드 4: ApexNav 핵심 기여 (2분)

**[화면: 4가지 핵심 기여점 요약]**

ApexNav는 다음 4가지 핵심 기여를 제시합니다.

**기여점 1: Target-Centric Semantic Fusion**

카메라의 시야각 중앙부가 가장자리보다 신뢰도가 높다는 관찰에서 출발합니다.

우리는 FOV 기반 신뢰도 가중치 모델을 개발했습니다:
```
C(p) = cos²((θ_rel / (FOV/2)) × (π/2))
```

이를 통해 시간에 따른 다중 관찰을 신뢰도 기반으로 융합함으로써 VLM 환각을 50% 이상 감소시킵니다.

**기여점 2: Adaptive Exploration Strategy**

4가지 탐색 정책을 상황에 따라 자동으로 전환합니다:
1. Distance-based: 의미론적 차이가 없을 때
2. Semantic-based: 특정 영역이 명확히 가치 높을 때
3. Hybrid: 거리와 의미론적 값의 가중 조합
4. TSP-Optimized: 다중 프론티어의 최적 투어

**기여점 3: Multi-Modal VLM 통합**

4개의 VLM을 앙상블합니다:
- GroundingDINO: 개방형 어휘 검출
- BLIP-2: 이미지-텍스트 매칭 검증
- Mobile-SAM: 빠른 인스턴스 분할
- YOLOv7: 제2 검증

이를 통해 단일 모델의 한계를 극복합니다.

**기여점 4: 강력한 상태 머신**

명확한 FSM으로 복잡한 동작을 체계적으로 관리하며, 6가지 상태와 7가지 탐색 모드를 정의하여 안정성을 보장합니다.

---

## 슬라이드 5: 시스템 아키텍처 (2분)

**[화면: Figure 2 - 전체 시스템 파이프라인 다이어그램]**

이제 전체 시스템 구조를 보여드리겠습니다.

**아키텍처의 3계층:**

**상단 - Habitat 인터페이스 (Python)**
- Habitat 시뮬레이터와의 통신
- RGB-D 센서 데이터 수집
- 액션 명령 전송

**중단 - ROS2 코어 (C++)**
이것이 ApexNav의 핵심입니다:
- ExplorationFSM: 상태 관리
- PlanningManager: 경로 계획 (A*, 동역학 제약, TSP)
- EnvironmentMap: 격자 맵, 의미론적 값, 프론티어, 객체 추적

**하단 - VLM 서버층 (Python)**
- 4개의 독립적인 VLM 서버
- HTTP-JSON RPC 통신으로 병렬 추론
- GPU 메모리 효율성

이 계층화된 구조는 다음과 같은 장점을 제공합니다:
- 메모리 격리
- 병렬 처리
- 확장 가능성
- 실시간 성능 (100 Hz)

---

## 슬라이드 6: Target-Centric Semantic Fusion (1분)

**[화면: Figure 5 - 융합 파이프라인 시각화]**

이제 가장 혁신적인 부분인 Target-Centric Semantic Fusion을 자세히 설명하겠습니다.

**기본 아이디어:**
사람이 물건을 찾을 때, 이미지의 가장자리보다 정중앙에서 본 것이 더 신뢰할 수 있다는 관찰입니다.

**구현:**

1단계: 각 프레임에서 BLIP-2로 이미지-텍스트 매칭 점수(ITM) 계산
- 범위: [0, 1]
- 점수 높음 = 객체 검출 신뢰도 높음

2단계: 카메라 시야각에서의 위치를 기반으로 신뢰도 계산
- 중심 (±39.5°): 신뢰도 = 1.0
- 가장자리: 신뢰도 = 0.0
- 공식: C(p) = cos²((θ_rel / (FOV/2)) × (π/2))

이 신뢰도 제곱을 사용하는 것이 중요합니다. 중심부를 강조하기 위해서입니다.

3단계: 시간에 따른 누적
```
C_new = (C_now² + C_last²) / (C_now + C_last)
V_new = (C_now × V_now + C_last × V_last) / (C_now + C_last)
```

이렇게 하면 같은 위치를 여러 각도에서 관찰할 때 신뢰도가 높아지고, 거짓 양성은 억제됩니다.

---

## 슬라이드 7: 할루시네이션 처리 (1분)

**[화면: Figure 6 - 실제 환각 처리 사례 연구]**

이 신뢰도 모델이 실제로 환각을 어떻게 처리하는지 보여드리겠습니다.

**사례 1: 검은색 가구와 그림자**
- 첫 관찰: 검은 영역에 높은 ITM 점수
- 두 번째 관찰 (다른 각도): 낮은 ITM 점수
- 결과: 누적 신뢰도는 중간~낮음으로 수렴
- 거짓 양성 제거됨

**사례 2: 실제 객체**
- 여러 각도에서 일관되게 높은 ITM 점수
- FOV 가중치로 인한 추가 강화
- 결과: 누적 신뢰도가 점진적으로 상승
- 신뢰할 수 있는 검출만 남음

이 메커니즘이 ApexNav의 **신뢰성** 향상의 핵심입니다.

---

## 슬라이드 8: Adaptive Exploration Strategy - 개요 (1분)

**[화면: Figure 4 - 적응형 탐색 전략 플로우]**

이제 두 번째 핵심 기여인 적응형 탐색 전략을 설명하겠습니다.

**왜 적응형인가?**

같은 탐색 정책을 모든 상황에 적용할 수 없습니다:
- 환경이 넓을 때: 거리 최소화가 중요
- 특정 영역에 객체가 집중될 때: 의미론적 값이 중요
- 복잡한 레이아웃: 경로 최적화가 중요

**4가지 탐색 정책:**

프론티어들의 의미론적 값 분포를 분석하여 자동으로 정책을 선택합니다:

```
표준 편차(σ)와 최대/평균 비율(r) 계산
     ↓
σ < 0.03 AND r < 1.2? → Distance-based
σ > 0.03 OR r > 1.2? → Semantic-based
그 외:
  - 프론티어가 많음? → TSP-Optimized
  - 프론티어가 적음? → Hybrid
```

이렇게 하면 상황에 최적의 정책을 자동으로 선택합니다.

---

## 슬라이드 9: Adaptive Exploration Strategy - 상세 (2분)

**[화면: 4가지 정책의 상세 비교 테이블]**

4가지 정책을 하나씩 살펴보겠습니다.

**정책 1: Distance-based (거리 기반)**
```
frontier_selected = argmin(d_i)
```
- 용도: 의미론적 가치 차이가 없을 때
- 장점: 가장 빠름, 계산 비용 낮음
- 예시: 초기 탐색 단계

**정책 2: Semantic-based (의미론적 기반)**
```
frontier_selected = argmax(V_i)
```
- 용도: 특정 프론티어의 가치가 명확히 높을 때
- 장점: 객체 발견 확률 최대화
- 예시: 객체 신호가 강할 때

**정책 3: Hybrid (하이브리드)**
```
cost_i = α × d_i - β × V_i
frontier_selected = argmin(cost_i)
```
- 용도: 거리와 의미론적 값의 균형 필요 시
- 장점: 유연한 조정
- 적응형: α와 β를 상황에 따라 동적 조정

**정책 4: TSP-Optimized (최적 투어)**
```
tour* = argmin(Σ d(tour_i, tour_{i+1}))
```
- 용도: 다중 프론티어의 효율적 방문
- 알고리즘: LKH (Lin-Kernighan Heuristic)
- 장점: 거의 최적해에 가까운 경로
- 시간 복잡도: 다항식 시간

이 4가지 정책의 조합으로 **효율성**을 달성합니다.

---

## 슬라이드 10: VLM 통합 아키텍처 (2분)

**[화면: VLM 서버 구조도 및 통신 프로토콜]**

이제 VLM 통합 부분을 설명하겠습니다.

ApexNav는 4개의 VLM을 HTTP-JSON RPC로 통신합니다.

**1. GroundingDINO (포트 12181)**
- 역할: 개방형 어휘 객체 검출
- 입력: 이미지 + 텍스트 설명 (예: "chair . person .")
- 출력: 바운딩 박스, 신뢰도, 클래스 이름
- 특징: 학습되지 않은 클래스도 검출 가능

**2. BLIP-2 ITM (포트 12182)**
- 역할: 이미지-텍스트 매칭 점수
- 입력: 이미지 + 텍스트 쿼리
- 출력: 매칭 점수 [0, 1]
- 특징: GroundingDINO 결과 재검증

**3. Mobile-SAM (포트 12183)**
- 역할: 빠른 인스턴스 분할
- 입력: 이미지 + 바운딩 박스
- 출력: 분할 마스크
- 특징: 경량 모델, 실시간 처리

**4. YOLOv7 (포트 12184)**
- 역할: 전통적 객체 검출 (제2 검증)
- 장점: 매우 빠른 속도, COCO 사전학습

**왜 4개인가?**

각 모델의 강점:
- GroundingDINO: 개방형 어휘, 새로운 객체
- BLIP-2: 미묘한 의미론적 구분
- Mobile-SAM: 정밀한 분할
- YOLOv7: 빠른 속도와 일반 객체

이들을 앙상블하면 개별 모델의 약점을 보완합니다.

**통신 프로토콜:**
```
Main Process (C++)
        ↓ HTTP Request (이미지 전송)
VLM Server (Python)
        ↓ GPU 추론
        ↓ HTTP Response (결과 반환)
Main Process (C++) ← 결과 처리
```

이 구조의 장점:
- 메모리 격리로 안정성 향상
- 병렬 추론 가능
- 개별 서버 재시작 가능

---

## 슬라이드 11: 경로 계획 (1분)

**[화면: 3단계 경로 계획 계층 다이어그램]**

ApexNav의 경로 계획은 3단계입니다.

**1단계: 격자 수준 - 2D A* Search**
- 목적: 충돌 회피 및 기본 경로 계획
- 시간 복잡도: O(n log n)
- 3가지 안전 모드:
  - NORMAL: 표준 마진 (0.15m)
  - OPTIMISTIC: 작은 마진 (0.10m) - 좁은 공간 용
  - EXTREME: 극소 마진 (0.05m) - 막힌 상황 탈출

**2단계: 동역학 제약 - Kinodynamic A***
- 목적: 로봇의 속도, 가속도 제약 고려
- 상태: [x, y, θ, v_x, v_y, ω]
- 제약: v_max, a_max, ω_max

**3단계: 궤적 평활화 - TSP + Path Shortcutting**
- LKH 솔버로 다중 웨이포인트 최적화
- Ray-casting으로 지그재그 경로 단축

이 3단계를 통해 계산 효율과 경로 품질을 모두 달성합니다.

---

## 슬라이드 12: 유한 상태 머신 (FSM) (1분)

**[화면: 6가지 주요 상태 다이어그램]**

ApexNav의 동작을 관리하는 FSM을 보여드리겠습니다.

**6가지 상태:**

1. **INIT**: 초기화 단계
   - 센서 보정, 맵 초기화

2. **WAIT_TRIGGER**: 트리거 대기
   - Habitat 시뮬레이터 시작 신호 대기

3. **PLAN_ACTION**: 액션 계획
   - 프론티어 검출, 경로 계획, 다음 액션 결정

4. **WAIT_ACTION_FINISH**: 액션 실행 완료 대기
   - 로봇이 액션을 완료할 때까지 대기

5. **PUB_ACTION**: 액션 발행
   - 계획된 액션을 Habitat에 전송

6. **FINISH**: 종료
   - 결과 정리 및 통계 수집

**실행 빈도:**
- 메인 루프: 100 Hz (0.01초)
- 프론티어 업데이트: 4 Hz (0.25초)

이 구조로 안정적이고 예측 가능한 동작을 보장합니다.

---

## 슬라이드 13: 탐색 모드와 의사 결정 (1분)

**[화면: 탐색 모드 전환 로직]**

FSM 내에서 탐색 결과에 따라 7가지 모드가 있습니다.

```
기본 모드:
├─ EXPLORATION (기본 탐색)
│
객체 발견 시:
├─ SEARCH_BEST_OBJECT (신뢰도 높은 객체 추격)
├─ SEARCH_OVER_DEPTH_OBJECT (먼 객체 추격)
├─ SEARCH_SUSPICIOUS_OBJECT (의심 객체 재확인)
│
극한 상황:
├─ NO_PASSABLE_FRONTIER (통과 가능한 경로 없음)
├─ NO_COVERABLE_FRONTIER (탐사 가능한 영역 없음)
└─ SEARCH_EXTREME (마지막 수단 - 극단적 탐색)
```

**모드 전환의 예시:**

신뢰도 > 0.8 → SEARCH_BEST_OBJECT로 즉시 전환하여 해당 객체 추격

누적 신뢰도 > 0.6 (하지만 확실하지 않음) → SEARCH_OVER_DEPTH_OBJECT로 더 가까이 접근하여 재확인

이런 동적 전환이 **안정성**을 제공합니다.

---

## 슬라이드 14: 실험 결과 (2분)

**[화면: Figure 7 - 성능 표 및 성공/실패 분석]**

이제 실험 결과를 보여드리겠습니다.

**실험 설정:**

데이터셋:
- HM3D v1: Habitat Matterport 3D 버전 1
- HM3D v2: Habitat Matterport 3D 버전 2
- MP3D: Matterport 3D

각 데이터셋에서 수십 개의 에피소드 실행

**평가 지표:**

| 지표 | 설명 |
|------|------|
| Success Rate | 목표 객체로부터 0.2m 이내 도달 비율 |
| SPL (Success weighted by Path Length) | 효율성을 고려한 성공률 |
| Path Length | 로봇이 이동한 실제 거리 |
| Time | 작업 완료 시간 |

**주요 성과:**

ApexNav vs VLFM 비교:
- Success Rate: **+15-25% 향상**
- SPL: **+10-20% 향상**
- Path Length: **-20-30% 감소**

구체적 수치:
- HM3D v2에서 Success Rate: 82% (VLFM 대비 +18%)
- SPL: 0.68 (VLFM 대비 +15%)

**실패 분석:**

성공하지 못한 경우:
1. 극도로 복잡한 환경: 프론티어 고갈
2. 매우 작은 객체: 거리가 멀 때 검출 불가
3. 환각이 반복되는 경우: 신뢰도 누적에도 시간 필요

이런 분석으로 향후 개선 방향을 제시합니다.

---

## 슬라이드 15: 성능 분석 (1분)

**[화면: 계산 복잡도 및 실시간 성능 차트]**

성능 측면에서의 분석입니다.

**계산 복잡도:**

```
프론티어 검출: O(n_grid) - 모든 그리드 순회
경로 계획 (A*): O(n_grid log n_grid) - 일반적 A*
적응형 선택: O(n_frontier) - 프론티어 통계
TSP 솔빙: O(n_frontier^3) (휴리스틱) - LKH 알고리즘
VLM 추론: O(1) - 병렬 처리로 시간 상수
```

**실시간 성능:**

메인 루프 100 Hz 달성:
- 각 사이클 평균 시간: ~80ms
- 최대 시간: ~120ms (경로 계획이 복잡할 때)
- 최소 시간: ~20ms (단순 상황)

**메모리 사용:**

- C++ 코어: ~150MB
- Python VLM 서버: GPU 메모리 기반
  - GroundingDINO: ~4GB
  - BLIP-2: ~3GB
  - Mobile-SAM: ~1GB
  - YOLOv7: ~1GB
  - 합계: ~9GB (병렬 공유 가능)

이 성능 특성으로 실제 로봇 탑재가 가능합니다.

---

## 슬라이드 16: 비교 분석 (1분)

**[화면: 기존 방법과의 비교 테이블]**

다른 방법들과 비교해보겠습니다.

| 특성 | Random | VLFM | ApexNav |
|------|--------|------|---------|
| **VLM 활용** | No | Yes | Yes (강화) |
| **신뢰도 모델** | - | No | FOV 기반 |
| **적응형 전략** | - | No | Yes (4가지) |
| **환각 처리** | - | 제한적 | 다중 누적 |
| **TSP 최적화** | - | No | Yes (LKH) |
| **실시간성** | 빠름 | 중간 | 빠름 (100Hz) |
| **Success Rate** | 40-50% | 65-75% | **80-90%** |
| **SPL** | 0.35-0.45 | 0.55-0.62 | **0.65-0.75** |

**ApexNav의 우위:**
1. 거의 2배의 성공률 향상 (Random 대비)
2. VLFM 대비 신뢰도 기반 환각 처리
3. 상황 인식 적응형 탐색
4. 실시간 성능 보장

---

## 슬라이드 17: 핵심 기여점 요약 (1분)

**[화면: 4가지 핵심 기여점 재정리]**

ApexNav의 4가지 핵심 기여를 다시 정리하겠습니다.

**1. Target-Centric Semantic Fusion (신뢰성)**
- FOV 기반 동적 가중치
- 신뢰도 제곱 기반 강화
- 시간 누적으로 VLM 환각 >50% 감소

**2. Adaptive Exploration Strategy (효율성)**
- 4가지 정책 자동 전환
- 상황 인식 의사 결정
- 경로 길이 20-30% 단축

**3. Multi-Modal VLM Integration (정확성)**
- 4개 VLM 앙상블
- 개별 모델 약점 보완
- Zero-shot 능력 강화

**4. Robust FSM & State Machine (안정성)**
- 명확한 상태 전이
- 6가지 상태, 7가지 탐색 모드
- 예측 가능한 동작 보장

이 4가지가 함께 작동하여 전체 성능을 극대화합니다.

---

## 슬라이드 18: 실제 적용 예시 (1분)

**[화면: 실제 환경에서의 시뮬레이션 화면]**

ApexNav가 실제로 어떻게 작동하는지 보여드리겠습니다.

**시나리오: "식탁을 찾아"**

1. **초기 탐색 (0-10초)**
   - 로봇이 주변을 스캔
   - 프론티어 여러 개 검출
   - Distance-based 정책 선택 (의미론적 신호 약함)
   - 가장 가까운 프론티어로 이동

2. **신호 발견 (10-15초)**
   - 어느 방향에서 높은 ITM 점수 검출
   - Semantic-based로 정책 전환
   - 그 방향 프론티어 우선순위 상향

3. **추격 (15-25초)**
   - 신뢰도 누적으로 0.7 달성
   - SEARCH_BEST_OBJECT 모드 진입
   - 식탁 방향으로 빠르게 접근

4. **도달 (25-28초)**
   - 식탁으로부터 0.2m 이내 도달
   - Success 판정

전체 성공 시간: **약 28초**
- VLFM: 약 45초
- Random: 60초 이상

이렇게 빠른 성공을 달성합니다.

---

## 슬라이드 19: 한계와 향후 연구 (1분)

**[화면: 한계 분석 및 미래 방향]**

ApexNav의 한계점과 향후 연구 방향을 정리하겠습니다.

**현재 한계:**

1. **시뮬레이션 환경**
   - Habitat 시뮬레이터에서만 평가됨
   - 실제 로봇 환경과의 갭 존재

2. **작은 객체**
   - 매우 작은 객체는 거리가 멀 때 검출 어려움
   - 고해상도 카메라 필요

3. **동적 환경**
   - 현재는 정적 환경 가정
   - 움직이는 사람, 객체는 미고려

4. **계산 비용**
   - 4개 VLM 병렬 실행에 ~9GB GPU 메모리 필요

**향후 연구 방향:**

1. **실제 로봇 배포**
   - Spot, HSR, 드론 등에 탑재
   - 실제 센서 데이터 처리

2. **동적 환경 대응**
   - 움직이는 객체 추적
   - 사람과의 상호작용

3. **멀티-에이전트**
   - 여러 로봇의 협력 탐색
   - 정보 공유 메커니즘

4. **경량화**
   - 엣지 디바이스 최적화
   - 모바일 로봇 탑재

5. **장기 자율 운영**
   - 배터리 제약 고려
   - 재충전 계획

---

## 슬라이드 20: 결론 및 의의 (1분)

**[화면: 결론 슬라이드 - 핵심 메시지]**

마지막으로 이 연구의 의미를 정리하겠습니다.

**ApexNav의 혁신성:**

ApexNav는 세 가지 주요 문제를 동시에 해결했습니다:

1. **VLM 환각의 체계적 해결**
   - FOV 기반 신뢰도 모델
   - 신뢰도 제곱 기반 누적
   - 거짓 양성 50% 이상 감소

2. **상황 인식 탐색 효율화**
   - 4가지 적응형 정책
   - TSP 최적화
   - 경로 길이 20-30% 단축

3. **Zero-Shot 능력의 실현**
   - 학습 없이 새로운 객체 검출
   - 다양한 환경에서 높은 성공률
   - 실제 로봇 응용 가능

**실제 임팩트:**

- 로봇이 사전 학습 없이 새로운 객체를 찾을 수 있음
- 실제 검색 및 구조, 배송 로봇 등에 직접 적용 가능
- 다양한 환경에 자동 적응하여 배포 비용 절감

**학술적 기여:**

- VLM 기반 네비게이션 연구에 새로운 방향 제시
- 신뢰도 기반 센서 융합의 일반화 가능 프레임워크
- 적응형 탐색 전략의 구체적 구현과 검증

**마무리:**

ApexNav는 비전-언어 모델을 활용한 로보틱스 분야에서 실질적인 진전을 이루었습니다.

앞으로 실제 로봇 플랫폼에 적용되고, 동적 환경 및 멀티-에이전트 협력으로 확장될 것으로 기대합니다.

감사합니다.

---

## Q&A

이제 여러분의 질문을 받겠습니다.

**예상 질문:**

Q1: "왜 4개의 VLM을 사용합니까? 1개만으로는 부족한가요?"
→ 각 모델이 다른 강점을 가져서 앙상블이 더 강력합니다. GroundingDINO는 개방형 어휘에 강하고, BLIP-2는 의미론적 구분에 강합니다.

Q2: "FOV 기반 신뢰도는 모든 카메라에 적용 가능한가요?"
→ 기본적으로 표준 FOV (79°)를 사용하지만, 카메라 스펙에 따라 조정 가능합니다.

Q3: "실제 로봇에는 언제 탑재 예정인가요?"
→ 현재 Habitat 시뮬레이터에서의 검증이 끝났고, Spot, HSR 등에 탑재 계획 중입니다.

Q4: "계산량이 많아 보이는데 GPU가 없으면 안 되나요?"
→ GPU는 필수적입니다. VLM 추론에 GPU 가속이 필수이며, CPU만으로는 실시간 성능 달성이 어렵습니다.

Q5: "다른 탐색 문제에도 적용 가능할까요?"
→ Target-Centric Semantic Fusion은 일반적 프레임워크로 다른 VLM 기반 로봇 작업에 적용 가능합니다.

질문이 있으시면 말씀해 주세요.

감사합니다.

---

## 발표 팁

### 시간 배분
- 슬라이드 1-5: 7분 (문제 정의 및 아키텍처)
- 슬라이드 6-11: 8분 (핵심 기여점 상세 설명)
- 슬라이드 12-17: 4분 (FSM 및 결과)
- 슬라이드 18-20: 1분 (예시 및 결론)

### 강조 포인트
1. **Target-Centric Semantic Fusion의 혁신성**: FOV 기반 신뢰도 모델이 단순하지만 효과적
2. **적응형 전략의 실용성**: 4가지 정책 자동 전환이 효율성 극대화
3. **실제 성과**: 15-25% 성능 향상은 실질적 의미
4. **확장 가능성**: 다른 로봇 작업에도 적용 가능

### 질문 예상 및 답변 준비
- VLM 모델 선택 이유
- FOV 기반 신뢰도의 일반화
- 실제 로봇 탑재 계획
- 계산 비용
- 다른 응용 분야

### 청중 참여 전략
- 문제 정의 단계에서 "당신이라면 어떻게 할까요?"라는 질문 제기
- 결과 해석 시 "왜 이런 결과가 나왔을까요?"라는 추론 유도
- 한계 논의에서 청중 의견 수렴

---

*이 발표 대본은 PhD 수준의 로보틱스/AI 연구자를 위해 작성되었습니다.*
*각 슬라이드는 원리, 구현, 결과를 균형 있게 설명하며 완전한 이해를 돕도록 구성되었습니다.*
